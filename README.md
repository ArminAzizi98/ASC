# Activation-Steered Compression (ASC)

**Activation-Steered Compression (ASC)** is a training-free method that compresses verbose reasoning in Large Language Models (LLMs) at inference time by manipulating internal activations. It achieves substantial reductions in Chain-of-Thought (CoT) length while preserving, or even improving, answer accuracy â€” enabling faster, more efficient, and cost-effective deployment of reasoning models.

> ğŸ“„ This repository accompanies our paper:  
> **Activation Steering for Chain-of-Thought Compression**

## ğŸš€ Overview

Chain-of-Thought prompting improves reasoning but often leads to:
- Verbose explanations
- Redundant reasoning steps
- Increased token usage and latency

ASC addresses this inefficiency by:
- Extracting a **steering vector** from paired verbose vs. concise rationales
- Injecting it into the modelâ€™s residual stream at inference time
- Compressing CoTs without retraining or fine-tuning

## ğŸ§  Key Features

- âš™ï¸ **Training-free**: Works on any model without parameter updates
- ğŸ’¡ **Concise reasoning**: Reduces CoT length by up to 67%
- âš¡ **Efficient inference**: Up to 2.73Ã— speedup in wall-clock time
- ğŸ§ª **Model-agnostic**: Works across 7B, 8B, and 32B parameter models
- ğŸ“ **Theoretical guarantees**: KL-bounded scaling ensures safe intervention

## ğŸ“Š Results Summary

| Model | Dataset | Accuracy (Î”) | Token Reduction |
|-------|---------|---------------|-----------------|
| DeepSeek-LLaMA-8B | MATH500 | â†”ï¸ (no drop) | â†“33.8% |
| DeepSeek-Qwen-7B  | GSM8K   | â†”ï¸           | â†“67.43% |
| QwQ-32B           | MATH500 | â†‘ +0.4%      | â†“50.7% |

## ğŸ› ï¸ Setup

```bash
git clone https://github.com/your-username/ASC.git
cd ASC
pip install -r requirements.txt
